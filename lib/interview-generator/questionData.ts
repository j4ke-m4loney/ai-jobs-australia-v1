export type Role =
  | "Machine Learning Engineer"
  | "Data Scientist"
  | "AI Researcher"
  | "Data Engineer"
  | "MLOps Engineer"
  | "AI Engineer"
  | "AI Governance Specialist"
  | "Software Developer - AI";

export type ExperienceLevel = "Junior" | "Mid" | "Senior" | "Lead";

export type InterviewStage =
  | "Phone Screen"
  | "Technical Interview"
  | "System Design"
  | "Behavioral"
  | "Final Round";

export type QuestionCategory =
  | "ML Fundamentals"
  | "Deep Learning"
  | "System Design"
  | "Coding & Algorithms"
  | "Statistics & Math"
  | "MLOps & Deployment"
  | "Domain Specific"
  | "Behavioral"
  | "Case Study";

export interface InterviewQuestion {
  id: string;
  question: string;
  category: QuestionCategory;
  difficulty: ExperienceLevel;
  roles: Role[];
  stages: InterviewStage[];
  sampleAnswer?: string;
  tips?: string;
}

// All roles available
export const ROLES: Role[] = [
  "Machine Learning Engineer",
  "Data Scientist",
  "AI Researcher",
  "Data Engineer",
  "MLOps Engineer",
  "AI Engineer",
  "AI Governance Specialist",
  "Software Developer - AI",
];

// Experience levels with descriptions
export const EXPERIENCE_LEVELS: { value: ExperienceLevel; label: string }[] = [
  { value: "Junior", label: "Junior (0-2 years)" },
  { value: "Mid", label: "Mid-Level (2-5 years)" },
  { value: "Senior", label: "Senior (5-10 years)" },
  { value: "Lead", label: "Lead/Principal (10+ years)" },
];

// Interview stages
export const INTERVIEW_STAGES: { value: InterviewStage; label: string }[] = [
  { value: "Phone Screen", label: "Phone Screen / Initial Call" },
  { value: "Technical Interview", label: "Technical Interview" },
  { value: "System Design", label: "System Design Round" },
  { value: "Behavioral", label: "Behavioral / Culture Fit" },
  { value: "Final Round", label: "Final Round / Leadership" },
];

// Question categories
export const QUESTION_CATEGORIES: QuestionCategory[] = [
  "ML Fundamentals",
  "Deep Learning",
  "System Design",
  "Coding & Algorithms",
  "Statistics & Math",
  "MLOps & Deployment",
  "Domain Specific",
  "Behavioral",
  "Case Study",
];

// Comprehensive question bank
export const INTERVIEW_QUESTIONS: InterviewQuestion[] = [
  // ML Fundamentals - Junior
  {
    id: "ml-1",
    question: "What is the difference between supervised and unsupervised learning? Give examples of each.",
    category: "ML Fundamentals",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher", "AI Engineer"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "Supervised learning uses labelled data to train models (e.g., classification, regression). Examples include spam detection and house price prediction. Unsupervised learning finds patterns in unlabelled data (e.g., clustering, dimensionality reduction). Examples include customer segmentation and anomaly detection.",
    tips: "Be ready to explain real-world applications you've worked on for each type.",
  },
  {
    id: "ml-2",
    question: "Explain the bias-variance tradeoff. How do you address it in practice?",
    category: "ML Fundamentals",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "Bias is error from oversimplified models (underfitting). Variance is error from models too sensitive to training data (overfitting). The tradeoff means reducing one often increases the other. Address it through cross-validation, regularisation, ensemble methods, and appropriate model complexity.",
    tips: "Draw a diagram showing training vs validation error curves to illustrate your point.",
  },
  {
    id: "ml-3",
    question: "What is cross-validation and why is it important?",
    category: "ML Fundamentals",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "Data Scientist"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "Cross-validation is a technique to assess model performance by splitting data into multiple folds, training on some and validating on others. K-fold CV trains K models, each time using a different fold as validation. It's important because it gives a more reliable performance estimate than a single train-test split, especially with limited data.",
    tips: "Mention when you might use stratified k-fold vs regular k-fold.",
  },
  {
    id: "ml-4",
    question: "How do you handle missing data in a dataset?",
    category: "ML Fundamentals",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "Data Scientist", "Data Engineer"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "Strategies include: 1) Deletion (listwise or pairwise) if missing data is random and small; 2) Imputation using mean, median, mode, or more sophisticated methods like KNN or MICE; 3) Using algorithms that handle missing values natively (XGBoost, LightGBM); 4) Creating a 'missing' indicator feature. The choice depends on the missing data mechanism (MCAR, MAR, MNAR).",
    tips: "Discuss how you'd investigate why data is missing before choosing a strategy.",
  },
  {
    id: "ml-5",
    question: "Explain the difference between L1 and L2 regularisation.",
    category: "ML Fundamentals",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher"],
    stages: ["Technical Interview"],
    sampleAnswer: "L1 (Lasso) adds absolute value of coefficients to loss, encouraging sparsity - useful for feature selection. L2 (Ridge) adds squared coefficients, shrinking weights toward zero but rarely to exactly zero. L1 produces sparse models, L2 handles multicollinearity better. Elastic Net combines both.",
    tips: "Be prepared to write the mathematical formulas and explain when to use each.",
  },

  // ML Fundamentals - Mid
  {
    id: "ml-6",
    question: "Explain gradient descent and its variants. When would you use each?",
    category: "ML Fundamentals",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher"],
    stages: ["Technical Interview"],
    sampleAnswer: "Batch GD uses entire dataset per update - stable but slow. Stochastic GD uses one sample - fast but noisy. Mini-batch GD balances both. Advanced variants: SGD with momentum adds velocity; Adam combines momentum with adaptive learning rates; RMSprop adapts learning rates per parameter. Use Adam as default for deep learning, SGD with momentum for well-tuned systems.",
    tips: "Discuss learning rate scheduling and how you'd debug slow convergence.",
  },
  {
    id: "ml-7",
    question: "How do you handle class imbalance in classification problems?",
    category: "ML Fundamentals",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "Data Scientist"],
    stages: ["Technical Interview"],
    sampleAnswer: "Techniques include: 1) Resampling - oversampling minority (SMOTE) or undersampling majority; 2) Class weights in loss function; 3) Threshold tuning for decision boundary; 4) Ensemble methods like balanced random forests; 5) Anomaly detection approach for extreme imbalance. Choose metrics carefully - use precision, recall, F1, AUC-PR over accuracy.",
    tips: "Mention specific thresholds or ratios where you'd switch between strategies.",
  },
  {
    id: "ml-8",
    question: "Explain how Random Forest works. What are its advantages and limitations?",
    category: "ML Fundamentals",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "Data Scientist"],
    stages: ["Technical Interview"],
    sampleAnswer: "Random Forest builds multiple decision trees using bootstrap sampling and random feature subsets, then aggregates predictions (voting or averaging). Advantages: handles non-linear relationships, robust to outliers, provides feature importance, low overfitting risk. Limitations: less interpretable than single trees, slower inference, doesn't extrapolate well, high memory for large forests.",
    tips: "Compare with gradient boosting and explain when you'd choose one over the other.",
  },

  // ML Fundamentals - Senior
  {
    id: "ml-9",
    question: "How would you approach building a model when you have very limited labelled data?",
    category: "ML Fundamentals",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher"],
    stages: ["Technical Interview", "System Design"],
    sampleAnswer: "Strategies: 1) Transfer learning - fine-tune pretrained models; 2) Semi-supervised learning - use unlabelled data; 3) Active learning - strategically select samples to label; 4) Data augmentation - create synthetic variations; 5) Few-shot or zero-shot learning with foundation models; 6) Weak supervision using labelling functions (Snorkel). Choice depends on domain and available resources.",
    tips: "Discuss cost-benefit analysis of labelling more data vs using these techniques.",
  },
  {
    id: "ml-10",
    question: "Describe how you would design an A/B test for a new ML model in production.",
    category: "ML Fundamentals",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "Data Scientist"],
    stages: ["Technical Interview", "System Design"],
    sampleAnswer: "Steps: 1) Define success metrics (primary and guardrail); 2) Calculate required sample size for statistical power; 3) Implement random traffic splitting; 4) Run for sufficient duration covering business cycles; 5) Monitor for sample ratio mismatch and novelty effects; 6) Analyse with appropriate statistical tests; 7) Consider segment analysis. Account for network effects if applicable.",
    tips: "Mention specific statistical tests you'd use and how you'd handle multiple comparisons.",
  },

  // Deep Learning - Junior
  {
    id: "dl-1",
    question: "What is a neural network activation function? Name three and explain when to use each.",
    category: "Deep Learning",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "AI Researcher"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "Activation functions introduce non-linearity. ReLU: max(0,x) - fast, works well for hidden layers but can have 'dying ReLU' problem. Sigmoid: maps to (0,1) - good for binary output but suffers from vanishing gradients. Softmax: normalises to probability distribution - used for multi-class output layer. Also: Tanh, Leaky ReLU, GELU (used in transformers).",
    tips: "Be ready to explain vanishing gradient problem and how different activations address it.",
  },
  {
    id: "dl-2",
    question: "Explain backpropagation in simple terms.",
    category: "Deep Learning",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "AI Researcher"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "Backpropagation calculates gradients of the loss with respect to each weight by applying the chain rule from output back to input. Forward pass computes predictions, then loss is calculated. Backward pass propagates error gradients layer by layer, updating weights to minimise loss. It's efficient because intermediate values from forward pass are reused.",
    tips: "Practice explaining this without jargon, as if to someone non-technical.",
  },
  {
    id: "dl-3",
    question: "What is dropout and why is it used?",
    category: "Deep Learning",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "AI Researcher"],
    stages: ["Technical Interview"],
    sampleAnswer: "Dropout randomly sets a fraction of neurons to zero during training, forcing the network to learn redundant representations. It acts as regularisation, reducing overfitting. At inference, all neurons are used but weights are scaled. Typical dropout rates are 0.2-0.5. It's like training an ensemble of networks that share weights.",
    tips: "Mention where dropout is typically placed (after activation) and when it might not help.",
  },

  // Deep Learning - Mid
  {
    id: "dl-4",
    question: "Explain the Transformer architecture and why it's been so successful.",
    category: "Deep Learning",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "AI Researcher", "AI Engineer"],
    stages: ["Technical Interview"],
    sampleAnswer: "Transformers use self-attention to weigh relationships between all positions in a sequence simultaneously, unlike RNNs which process sequentially. Key components: multi-head attention, positional encoding, feed-forward layers, layer normalisation. Success factors: parallelisable training, captures long-range dependencies, scales well with data and compute. Basis for BERT, GPT, and most modern LLMs.",
    tips: "Be ready to explain attention mechanism mathematically: Q, K, V matrices and softmax.",
  },
  {
    id: "dl-5",
    question: "What techniques would you use to prevent overfitting in deep learning?",
    category: "Deep Learning",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "AI Researcher"],
    stages: ["Technical Interview"],
    sampleAnswer: "Techniques: 1) Regularisation - L2, dropout, batch normalisation; 2) Data augmentation - especially for images; 3) Early stopping based on validation loss; 4) Reduce model capacity if data is limited; 5) Transfer learning from pretrained models; 6) Cross-validation; 7) Noise injection; 8) Label smoothing. Monitor train vs validation curves to diagnose.",
    tips: "Discuss how you'd systematically try these techniques and measure their impact.",
  },
  {
    id: "dl-6",
    question: "Explain batch normalisation. Where is it placed and why does it help?",
    category: "Deep Learning",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "AI Researcher"],
    stages: ["Technical Interview"],
    sampleAnswer: "Batch norm normalises layer inputs to zero mean and unit variance across the batch, then applies learnable scale and shift. Typically placed before or after activation. Benefits: allows higher learning rates, reduces internal covariate shift, acts as regularisation, makes network less sensitive to initialisation. Note: different behavior at train vs inference time.",
    tips: "Mention alternatives like layer norm, group norm, and when you'd choose each.",
  },

  // Deep Learning - Senior
  {
    id: "dl-7",
    question: "How would you approach fine-tuning a large language model for a specific domain?",
    category: "Deep Learning",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "AI Researcher", "AI Engineer"],
    stages: ["Technical Interview", "System Design"],
    sampleAnswer: "Approaches: 1) Full fine-tuning on domain data (expensive); 2) Parameter-efficient methods - LoRA, adapters, prefix tuning; 3) Prompt engineering and in-context learning; 4) Retrieval-augmented generation (RAG). Consider: data quality and size, compute budget, latency requirements, whether to use instruction tuning or RLHF. Evaluate on domain-specific benchmarks.",
    tips: "Discuss practical considerations: catastrophic forgetting, evaluation metrics, deployment costs.",
  },
  {
    id: "dl-8",
    question: "Explain how you would debug a neural network that's not learning.",
    category: "Deep Learning",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "AI Researcher"],
    stages: ["Technical Interview"],
    sampleAnswer: "Systematic approach: 1) Verify data pipeline - check labels, normalization, augmentation; 2) Overfit on tiny batch first; 3) Check gradients - look for vanishing/exploding; 4) Verify loss function is appropriate; 5) Start with simpler model and known hyperparameters; 6) Visualise activations and gradients layer by layer; 7) Check learning rate - try learning rate finder; 8) Ensure reproducibility with seeds.",
    tips: "Share a specific debugging war story if you have one.",
  },

  // System Design - Mid
  {
    id: "sd-1",
    question: "Design a recommendation system for an e-commerce platform.",
    category: "System Design",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "Data Scientist"],
    stages: ["System Design"],
    sampleAnswer: "Components: 1) Candidate generation - collaborative filtering, content-based, or ANN for embeddings; 2) Ranking model - combines user, item, context features; 3) Real-time layer for session-based recommendations; 4) A/B testing framework. Considerations: cold start (use content features), scalability (approximate nearest neighbours), freshness (balance exploration/exploitation), latency requirements.",
    tips: "Start with clarifying questions about scale, latency requirements, and business metrics.",
  },
  {
    id: "sd-2",
    question: "How would you design a real-time fraud detection system?",
    category: "System Design",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "Data Scientist", "Data Engineer"],
    stages: ["System Design"],
    sampleAnswer: "Architecture: 1) Stream processing (Kafka/Flink) for real-time features; 2) Feature store combining real-time and batch features; 3) Low-latency model serving (<100ms); 4) Rules engine for known patterns; 5) Feedback loop for labels. Challenges: extreme class imbalance, adversarial drift, explainability requirements, balancing false positives with user friction.",
    tips: "Discuss how you'd handle the trade-off between blocking legitimate transactions and catching fraud.",
  },

  // System Design - Senior
  {
    id: "sd-3",
    question: "Design an end-to-end ML platform for a company with multiple data science teams.",
    category: "System Design",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "MLOps Engineer"],
    stages: ["System Design", "Final Round"],
    sampleAnswer: "Components: 1) Feature store (online + offline); 2) Experiment tracking (MLflow/W&B); 3) Training infrastructure (distributed, GPU scheduling); 4) Model registry with versioning; 5) Serving infrastructure (batch + real-time); 6) Monitoring and alerting; 7) CI/CD pipelines; 8) Data versioning. Consider: self-service vs centralised, cost management, governance, multi-tenancy.",
    tips: "Draw the architecture and explain data flows. Discuss build vs buy decisions.",
  },
  {
    id: "sd-4",
    question: "Design a system to serve a large language model at scale with low latency.",
    category: "System Design",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "MLOps Engineer"],
    stages: ["System Design"],
    sampleAnswer: "Considerations: 1) Model optimisation - quantization, distillation, pruning; 2) Inference optimisation - batching, KV caching, speculative decoding; 3) Infrastructure - GPU selection, multi-GPU inference; 4) Serving layer - load balancing, auto-scaling, request queuing; 5) Caching common responses; 6) Streaming responses for better perceived latency. Trade-offs: cost vs latency vs quality.",
    tips: "Quantify with numbers: expected QPS, acceptable latency percentiles, cost per request.",
  },

  // Coding & Algorithms - Junior
  {
    id: "code-1",
    question: "Write a function to implement k-nearest neighbours from scratch.",
    category: "Coding & Algorithms",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "Data Scientist"],
    stages: ["Technical Interview"],
    sampleAnswer: "Key steps: 1) Calculate distances between query point and all training points (Euclidean or other metric); 2) Sort by distance; 3) Select k nearest; 4) For classification: majority vote. For regression: average. Optimisation: use KD-tree or ball tree for large datasets.",
    tips: "Discuss time complexity O(nd) for brute force and how data structures can improve it.",
  },
  {
    id: "code-2",
    question: "Implement a function to calculate precision, recall, and F1 score.",
    category: "Coding & Algorithms",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "Data Scientist"],
    stages: ["Technical Interview"],
    sampleAnswer: "Given predictions and ground truth: TP = true positives, FP = false positives, FN = false negatives. Precision = TP/(TP+FP), Recall = TP/(TP+FN), F1 = 2*(Precision*Recall)/(Precision+Recall). Handle edge cases: division by zero when no positive predictions.",
    tips: "Extend to multi-class: micro, macro, and weighted averaging.",
  },

  // Coding & Algorithms - Mid
  {
    id: "code-3",
    question: "Implement gradient descent for linear regression.",
    category: "Coding & Algorithms",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher"],
    stages: ["Technical Interview"],
    sampleAnswer: "Steps: 1) Initialise weights; 2) Loop: compute predictions, calculate MSE loss, compute gradients dL/dw = -2/n * X.T @ (y - predictions), update weights w = w - lr * gradient; 3) Repeat until convergence. Include bias term. Consider normalising features for faster convergence.",
    tips: "Discuss vectorised implementation vs loops, and stopping criteria.",
  },
  {
    id: "code-4",
    question: "Write code to implement a simple neural network layer forward and backward pass.",
    category: "Coding & Algorithms",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "AI Researcher"],
    stages: ["Technical Interview"],
    sampleAnswer: "Forward: output = activation(X @ W + b). Backward: compute gradient of loss w.r.t. output, then: dL/dW = X.T @ (dL/dout * activation_derivative), dL/db = sum(dL/dout * activation_derivative), dL/dX for upstream gradient. Cache intermediate values from forward pass.",
    tips: "Be ready to extend to convolutional or attention layers.",
  },

  // Statistics & Math - Junior
  {
    id: "stats-1",
    question: "Explain the Central Limit Theorem and its importance in machine learning.",
    category: "Statistics & Math",
    difficulty: "Junior",
    roles: ["Data Scientist", "AI Researcher"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "CLT states that the sampling distribution of the mean approaches normal distribution as sample size increases, regardless of population distribution. Important for: confidence intervals, hypothesis testing, justifying normality assumptions in some ML algorithms, understanding why ensemble methods work (averaging predictions).",
    tips: "Give a practical example of when you relied on CLT in your work.",
  },
  {
    id: "stats-2",
    question: "What is the difference between correlation and causation? Give an example.",
    category: "Statistics & Math",
    difficulty: "Junior",
    roles: ["Data Scientist", "AI Researcher"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "Correlation measures statistical relationship between variables; causation means one variable directly influences another. Example: ice cream sales and drowning deaths are correlated (both increase in summer) but one doesn't cause the other - temperature is a confounding variable. Establishing causation requires controlled experiments or causal inference methods.",
    tips: "Mention techniques for causal inference: randomised experiments, instrumental variables, difference-in-differences.",
  },

  // Statistics & Math - Mid
  {
    id: "stats-3",
    question: "Explain Bayes' theorem and give an example of its application in ML.",
    category: "Statistics & Math",
    difficulty: "Mid",
    roles: ["Data Scientist", "AI Researcher"],
    stages: ["Technical Interview"],
    sampleAnswer: "Bayes' theorem: P(A|B) = P(B|A) * P(A) / P(B). Updates prior beliefs with evidence. ML applications: Naive Bayes classifiers, Bayesian neural networks, probabilistic programming, A/B test analysis, spam filtering. Example: updating belief that email is spam given certain words appear.",
    tips: "Be ready to work through a numerical example on the whiteboard.",
  },
  {
    id: "stats-4",
    question: "What is maximum likelihood estimation? How does it relate to loss functions?",
    category: "Statistics & Math",
    difficulty: "Mid",
    roles: ["Data Scientist", "AI Researcher"],
    stages: ["Technical Interview"],
    sampleAnswer: "MLE finds parameters that maximise the probability of observed data. For Gaussian errors, MLE gives same solution as minimising MSE. For Bernoulli outcomes, MLE equivalent to minimising cross-entropy loss. Connection: negative log-likelihood is the loss function. This unifies statistical and ML perspectives on model fitting.",
    tips: "Show the mathematical derivation for at least one case.",
  },

  // MLOps & Deployment - Junior
  {
    id: "mlops-1",
    question: "What is the difference between batch and real-time inference? When would you use each?",
    category: "MLOps & Deployment",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "MLOps Engineer"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "Batch: process large amounts of data periodically (hourly/daily). Good for non-time-sensitive predictions, training data generation, analytics. Real-time: predictions on-demand with low latency (<100ms). Good for user-facing features, fraud detection, recommendations. Considerations: infrastructure cost, latency requirements, freshness needs.",
    tips: "Discuss hybrid approaches: near-real-time with micro-batching.",
  },
  {
    id: "mlops-2",
    question: "Explain what model versioning is and why it's important.",
    category: "MLOps & Deployment",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "MLOps Engineer", "Data Scientist"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "Model versioning tracks different iterations of models along with their code, data, hyperparameters, and performance metrics. Important for: reproducibility, rollback capability, A/B testing, auditing, collaboration. Tools: MLflow, DVC, Weights & Biases. Should version: model artifacts, training code, config, data lineage.",
    tips: "Mention how you've implemented versioning in past projects.",
  },

  // MLOps & Deployment - Mid
  {
    id: "mlops-3",
    question: "How do you monitor ML models in production? What metrics would you track?",
    category: "MLOps & Deployment",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "MLOps Engineer"],
    stages: ["Technical Interview", "System Design"],
    sampleAnswer: "Monitor: 1) Technical metrics - latency, throughput, errors; 2) Model performance - accuracy metrics if labels available; 3) Data drift - input feature distributions; 4) Prediction drift - output distributions; 5) Business metrics - conversion, engagement. Set up alerts for anomalies. Use statistical tests (KS test, PSI) for drift detection.",
    tips: "Discuss how quickly you can detect issues and what your incident response process looks like.",
  },
  {
    id: "mlops-4",
    question: "Explain the concept of feature stores. What problems do they solve?",
    category: "MLOps & Deployment",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "MLOps Engineer", "Data Engineer"],
    stages: ["Technical Interview", "System Design"],
    sampleAnswer: "Feature stores provide centralised repository for feature definitions, computation, and serving. Solve: 1) Training-serving skew by using same features; 2) Feature reuse across teams; 3) Point-in-time correctness for training data; 4) Low-latency serving for online features. Components: offline store (batch), online store (real-time), feature registry. Examples: Feast, Tecton.",
    tips: "Discuss trade-offs between building vs buying a feature store.",
  },

  // MLOps & Deployment - Senior
  {
    id: "mlops-5",
    question: "How would you implement a robust CI/CD pipeline for ML models?",
    category: "MLOps & Deployment",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "MLOps Engineer"],
    stages: ["Technical Interview", "System Design"],
    sampleAnswer: "Pipeline stages: 1) Code quality checks (linting, tests); 2) Data validation; 3) Model training with experiment tracking; 4) Model validation against baseline and quality gates; 5) Staging deployment with shadow mode; 6) Canary deployment to production; 7) Monitoring and automated rollback. Include integration tests for feature pipelines and model serving.",
    tips: "Discuss how you'd handle failures at each stage and what manual gates you'd include.",
  },
  {
    id: "mlops-6",
    question: "Describe strategies for handling model retraining and when to trigger it.",
    category: "MLOps & Deployment",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "MLOps Engineer"],
    stages: ["Technical Interview", "System Design"],
    sampleAnswer: "Triggers: 1) Scheduled (daily/weekly) if data volume is high; 2) Performance degradation detected; 3) Significant data drift; 4) New labelled data available. Strategies: full retrain vs incremental learning, warm starting, continual learning. Consider: computational cost, label availability, model stability. Always validate before promoting to production.",
    tips: "Discuss how you'd balance freshness vs stability, especially for critical systems.",
  },

  // Behavioral - All Levels
  {
    id: "behav-1",
    question: "Tell me about a time when you had to explain a complex ML concept to a non-technical stakeholder.",
    category: "Behavioral",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher", "Data Engineer", "MLOps Engineer", "AI Engineer", "AI Governance Specialist", "Software Developer - AI"],
    stages: ["Behavioral", "Final Round"],
    sampleAnswer: "Use STAR format: Describe the Situation, Task you faced, Actions you took (analogies, visualisations, focusing on business impact), and Results achieved. Emphasise listening to understand their concerns and adapting your communication style.",
    tips: "Prepare 2-3 specific examples with quantified outcomes.",
  },
  {
    id: "behav-2",
    question: "Describe a situation where your model didn't perform as expected in production. How did you handle it?",
    category: "Behavioral",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "Data Scientist", "MLOps Engineer"],
    stages: ["Behavioral", "Final Round"],
    sampleAnswer: "Structure: 1) Detection - how you found out; 2) Investigation - root cause analysis (data drift, edge cases, concept drift); 3) Mitigation - short-term fixes; 4) Long-term solution - monitoring improvements, retraining strategy; 5) Learnings - what you'd do differently. Show ownership and systematic problem-solving.",
    tips: "Be honest about failures - interviewers value learning from mistakes.",
  },
  {
    id: "behav-3",
    question: "How do you prioritise between model accuracy and deployment speed?",
    category: "Behavioral",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "Data Scientist", "MLOps Engineer"],
    stages: ["Behavioral", "Final Round"],
    sampleAnswer: "Discuss: understanding business requirements first, baseline models for quick wins, iterative improvement, defining minimum viable performance, technical debt trade-offs, A/B testing to validate incrementally. Emphasise collaboration with stakeholders to align on priorities and timelines.",
    tips: "Use specific examples where you made these trade-offs.",
  },
  {
    id: "behav-4",
    question: "Tell me about a time you disagreed with a technical decision. How did you handle it?",
    category: "Behavioral",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher", "Data Engineer", "MLOps Engineer", "AI Engineer", "AI Governance Specialist", "Software Developer - AI"],
    stages: ["Behavioral", "Final Round"],
    sampleAnswer: "Show: 1) You understood the other perspective; 2) Presented your view with evidence; 3) Remained open to being wrong; 4) Found constructive resolution (compromise, experiment, or commit and disagree). Emphasise relationship preservation and focus on outcomes over ego.",
    tips: "Avoid speaking negatively about colleagues. Focus on the process and learning.",
  },
  {
    id: "behav-5",
    question: "How do you stay current with the rapidly evolving AI/ML field?",
    category: "Behavioral",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher", "Data Engineer", "MLOps Engineer"],
    stages: ["Phone Screen", "Behavioral"],
    sampleAnswer: "Mention: reading papers (arXiv, conference proceedings), following researchers on Twitter/social media, attending meetups and conferences, hands-on experimentation, online courses, open source contributions, discussing with peers. Be specific about recent papers or techniques you've explored.",
    tips: "Reference something specific you learned recently and how you applied it.",
  },

  // Case Study - Mid
  {
    id: "case-1",
    question: "You notice your production model's performance has degraded by 15% over the past month. Walk me through your investigation.",
    category: "Case Study",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "Data Scientist", "MLOps Engineer"],
    stages: ["Technical Interview", "Final Round"],
    sampleAnswer: "Investigation steps: 1) Verify metrics are correct (instrumentation issues?); 2) Check for data quality issues (missing features, pipeline failures); 3) Analyse input data drift using statistical tests; 4) Segment analysis - is degradation uniform or specific to subgroups?; 5) Check for concept drift if labels available; 6) Review any upstream changes (data sources, preprocessing); 7) Compare to baseline model.",
    tips: "Walk through your thought process systematically. Ask clarifying questions.",
  },
  {
    id: "case-2",
    question: "Your company wants to build a churn prediction model. How would you approach this project end-to-end?",
    category: "Case Study",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "Data Scientist"],
    stages: ["Technical Interview", "System Design"],
    sampleAnswer: "Steps: 1) Define churn (timeframe, criteria) with stakeholders; 2) Explore data, understand features; 3) Feature engineering (usage patterns, engagement, demographics); 4) Handle class imbalance; 5) Model selection and training; 6) Evaluate with business-relevant metrics; 7) Deployment strategy (batch scores vs real-time); 8) Integration with retention campaigns; 9) Monitor and iterate.",
    tips: "Emphasise business alignment and how you'd measure impact.",
  },

  // Case Study - Senior
  {
    id: "case-3",
    question: "Design an ML solution to improve search relevance for a job board platform.",
    category: "Case Study",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "Data Scientist"],
    stages: ["System Design", "Final Round"],
    sampleAnswer: "Approach: 1) Define success metrics (click-through, apply rate, time to apply); 2) Baseline with BM25/TF-IDF; 3) Learning to rank with features: query-job match, user history, job freshness, location; 4) Embedding-based semantic search with BERT; 5) Personalisation using user behaviour; 6) Query understanding (intent, entities); 7) A/B testing framework; 8) Handle cold start and long-tail queries.",
    tips: "This is directly relevant to the platform - show domain understanding.",
  },
  {
    id: "case-4",
    question: "How would you build a system to automatically tag and categorise job postings?",
    category: "Case Study",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher"],
    stages: ["Technical Interview", "System Design"],
    sampleAnswer: "Components: 1) Define taxonomy (skills, industries, seniority, work type); 2) Multi-label classification approach; 3) Data: use existing tags for training, augment with LLM-generated labels; 4) Model: fine-tuned BERT or similar; 5) Handle taxonomy evolution and new categories; 6) Human-in-the-loop for quality assurance; 7) Confidence thresholds for automated vs manual review.",
    tips: "Discuss trade-offs between LLM-based classification vs traditional ML approaches.",
  },

  // Domain Specific - Various
  {
    id: "domain-1",
    question: "How would you approach building an NLP pipeline for extracting skills from resumes?",
    category: "Domain Specific",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher"],
    stages: ["Technical Interview", "System Design"],
    sampleAnswer: "Approach: 1) Named Entity Recognition for skill extraction; 2) Use pretrained models (spaCy, BERT) fine-tuned on resume data; 3) Skill taxonomy for normalisation; 4) Handle variations (Python vs Python3 vs Python programming); 5) Context matters (used Python vs learning Python); 6) Combine with rule-based patterns for common formats; 7) Continuous improvement with user feedback.",
    tips: "Mention specific challenges: abbreviations, synonyms, context dependency.",
  },
  {
    id: "domain-2",
    question: "Explain how you would implement RAG (Retrieval-Augmented Generation) for a knowledge base Q&A system.",
    category: "Domain Specific",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "AI Researcher", "AI Engineer", "Software Developer - AI"],
    stages: ["Technical Interview", "System Design"],
    sampleAnswer: "Components: 1) Document processing and chunking strategy; 2) Embedding model for semantic search; 3) Vector database (Pinecone, Weaviate, Chroma); 4) Retrieval with hybrid search (dense + sparse); 5) Context window management; 6) LLM for generation with retrieved context; 7) Re-ranking retrieved documents; 8) Evaluation: retrieval quality and generation faithfulness. Handle: hallucination, attribution, freshness.",
    tips: "Discuss recent advancements like HyDE, self-RAG, or graph-based approaches.",
  },
  {
    id: "domain-3",
    question: "How would you approach building a computer vision model to detect defects in manufacturing?",
    category: "Domain Specific",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "AI Researcher"],
    stages: ["Technical Interview", "System Design"],
    sampleAnswer: "Approach: 1) Data collection - consistent imaging conditions; 2) Annotation strategy - bounding boxes or segmentation; 3) Handle class imbalance (defects are rare); 4) Transfer learning from pretrained models; 5) Data augmentation relevant to domain; 6) Anomaly detection if labelled defects are scarce; 7) Deployment on edge devices for real-time inspection; 8) Continuous learning with production feedback.",
    tips: "Discuss practical constraints: lighting, camera position, inference speed.",
  },

  // Lead-level questions
  {
    id: "lead-1",
    question: "How do you evaluate whether a business problem is suitable for an ML solution?",
    category: "Case Study",
    difficulty: "Lead",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher", "MLOps Engineer", "AI Engineer", "AI Governance Specialist"],
    stages: ["Final Round"],
    sampleAnswer: "Framework: 1) Is there a clear business value and metric? 2) Is the problem predictive in nature? 3) Is data available and of sufficient quality? 4) Are patterns learnable? (baseline analysis); 5) Can we get feedback to improve? 6) Is ML complexity justified vs simpler solutions? 7) Are there ethical/bias concerns? 8) Can we maintain it long-term? Sometimes the answer is 'start with rules'.",
    tips: "Show business acumen alongside technical judgment.",
  },
  {
    id: "lead-2",
    question: "How do you build and grow an ML team? What qualities do you look for in ML engineers?",
    category: "Behavioral",
    difficulty: "Lead",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher", "MLOps Engineer"],
    stages: ["Final Round"],
    sampleAnswer: "Team building: Balance of skills (research vs engineering vs ops), psychological safety, clear ownership, mentorship culture. Qualities: strong fundamentals over tool knowledge, curiosity and learning agility, communication skills, pragmatism, collaboration. Structure: avoid silos between research and production, embedded vs centralised teams trade-offs.",
    tips: "Share specific examples of people you've hired or mentored.",
  },
  {
    id: "lead-3",
    question: "How do you approach technical debt in ML systems?",
    category: "MLOps & Deployment",
    difficulty: "Lead",
    roles: ["Machine Learning Engineer", "MLOps Engineer"],
    stages: ["Technical Interview", "Final Round"],
    sampleAnswer: "ML-specific debt: glue code, pipeline jungles, dead experimental paths, undeclared consumers, feedback loops, configuration debt. Approach: 1) Track and make visible; 2) Allocate dedicated time (e.g., 20%); 3) Pay down incrementally; 4) Prevent accumulation through standards and review; 5) Balance with velocity. Prioritise based on risk and maintenance burden.",
    tips: "Reference the 'Machine Learning: The High-Interest Credit Card of Technical Debt' paper.",
  },

  // Additional ML Fundamentals
  {
    id: "ml-11",
    question: "What is feature engineering and why is it important?",
    category: "ML Fundamentals",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "Data Scientist", "Data Engineer"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "Feature engineering is the process of using domain knowledge to create, transform, or select input variables that improve model performance. It's important because models are only as good as their features - even simple models with great features often outperform complex models with poor features. Examples include creating interaction terms, binning continuous variables, and extracting date components.",
    tips: "Share a specific example where feature engineering significantly improved your model's performance.",
  },
  {
    id: "ml-12",
    question: "Explain the difference between parametric and non-parametric models.",
    category: "ML Fundamentals",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "Parametric models have a fixed number of parameters regardless of data size (e.g., linear regression, logistic regression). They make strong assumptions about data distribution. Non-parametric models grow in complexity with data (e.g., KNN, decision trees, kernel SVM). They make fewer assumptions but may require more data and computation.",
    tips: "Discuss trade-offs: parametric models are faster but may underfit; non-parametric are flexible but may overfit.",
  },
  {
    id: "ml-13",
    question: "What is ensemble learning? Describe different ensemble methods.",
    category: "ML Fundamentals",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "Data Scientist"],
    stages: ["Technical Interview"],
    sampleAnswer: "Ensemble learning combines multiple models to improve performance. Types: 1) Bagging - trains models on bootstrap samples and averages predictions (Random Forest); 2) Boosting - trains models sequentially, each focusing on previous errors (XGBoost, AdaBoost); 3) Stacking - uses a meta-model to combine base model predictions. Ensembles reduce variance (bagging) or bias (boosting).",
    tips: "Be ready to explain when you'd choose bagging vs boosting based on your data characteristics.",
  },
  {
    id: "ml-14",
    question: "How do you select the right evaluation metric for a machine learning problem?",
    category: "ML Fundamentals",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "Data Scientist"],
    stages: ["Technical Interview"],
    sampleAnswer: "Metric selection depends on the business problem. Classification: accuracy for balanced classes; precision when false positives are costly (spam); recall when false negatives are costly (disease detection); F1 for balance; AUC-ROC for ranking. Regression: MSE penalises large errors; MAE for interpretability; MAPE for percentage errors. Always align metrics with business objectives.",
    tips: "Discuss how you'd translate business requirements into specific metric choices.",
  },
  {
    id: "ml-15",
    question: "What is the curse of dimensionality and how do you address it?",
    category: "ML Fundamentals",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher"],
    stages: ["Technical Interview"],
    sampleAnswer: "As dimensions increase, data becomes sparse - distances between points become similar, making it hard to find patterns. Solutions: 1) Dimensionality reduction (PCA, t-SNE, autoencoders); 2) Feature selection to remove irrelevant features; 3) Regularisation; 4) Collect more data; 5) Use algorithms that handle high dimensions well (tree-based methods). Rule of thumb: need exponentially more data as dimensions grow.",
    tips: "Provide a concrete example of when you encountered this and how you solved it.",
  },

  // Additional Deep Learning
  {
    id: "dl-9",
    question: "What are CNNs and what problems are they best suited for?",
    category: "Deep Learning",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "AI Researcher"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "Convolutional Neural Networks use convolutional layers that apply learnable filters to detect local patterns, followed by pooling to reduce dimensionality. They exploit spatial hierarchies - early layers detect edges, later layers detect complex features. Best for: image classification, object detection, segmentation, and any data with spatial/local structure (audio spectrograms, time series).",
    tips: "Be prepared to draw a simple CNN architecture and explain each layer's purpose.",
  },
  {
    id: "dl-10",
    question: "What are RNNs and LSTMs? When would you use them over Transformers?",
    category: "Deep Learning",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "AI Researcher"],
    stages: ["Technical Interview"],
    sampleAnswer: "RNNs process sequences by maintaining hidden state across timesteps. LSTMs add gating mechanisms to handle long-range dependencies and vanishing gradients. Use over Transformers when: 1) Memory/compute is limited; 2) Sequence length is very long; 3) Streaming/online processing is needed; 4) Data is limited. Transformers are preferred for most NLP tasks now due to parallelisation and performance.",
    tips: "Mention specific architectures like bidirectional LSTMs and attention-augmented RNNs.",
  },
  {
    id: "dl-11",
    question: "Explain the concept of attention mechanism in neural networks.",
    category: "Deep Learning",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "AI Researcher"],
    stages: ["Technical Interview"],
    sampleAnswer: "Attention allows models to focus on relevant parts of input when producing output. Self-attention computes relevance scores between all positions in a sequence. Formula: Attention(Q,K,V) = softmax(QK^T/sqrt(d_k))V. Benefits: captures long-range dependencies, parallelisable, interpretable via attention weights. Multi-head attention allows attending to different aspects simultaneously.",
    tips: "Draw the attention computation diagram and explain Q, K, V matrices.",
  },
  {
    id: "dl-12",
    question: "What is transfer learning and when should you use it?",
    category: "Deep Learning",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher", "AI Engineer", "Software Developer - AI"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "Transfer learning uses a model trained on one task as the starting point for another task. Common approach: freeze early layers (generic features), fine-tune later layers (task-specific). Use when: 1) Limited labelled data; 2) Similar source and target domains; 3) Faster training needed. Examples: ImageNet pretrained CNNs for medical imaging, BERT for domain-specific NLP.",
    tips: "Discuss strategies for deciding how many layers to freeze vs fine-tune.",
  },
  {
    id: "dl-13",
    question: "How do GANs work? What are their applications and challenges?",
    category: "Deep Learning",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "AI Researcher"],
    stages: ["Technical Interview"],
    sampleAnswer: "GANs consist of a Generator (creates fake samples) and Discriminator (distinguishes real from fake), trained adversarially. Generator minimises Discriminator's accuracy; Discriminator maximises it. Applications: image generation, style transfer, data augmentation, super-resolution. Challenges: mode collapse, training instability, evaluation difficulty. Variants: DCGAN, StyleGAN, CycleGAN address various issues.",
    tips: "Mention recent alternatives like diffusion models and when you'd choose GANs vs diffusion.",
  },
  {
    id: "dl-14",
    question: "Explain the architecture and training of large language models like GPT.",
    category: "Deep Learning",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "AI Researcher"],
    stages: ["Technical Interview"],
    sampleAnswer: "GPT uses decoder-only Transformer with causal (left-to-right) attention. Training: 1) Pretraining on large text corpus with next-token prediction; 2) Optional instruction tuning on curated datasets; 3) RLHF for alignment. Key innovations: scaling laws, in-context learning, emergent abilities. Architecture details: layer normalisation, positional embeddings, attention heads, feed-forward layers.",
    tips: "Discuss practical considerations: context length, tokenisation, inference optimisation.",
  },

  // Additional System Design
  {
    id: "sd-5",
    question: "Design a content moderation system using ML.",
    category: "System Design",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "Data Scientist"],
    stages: ["System Design"],
    sampleAnswer: "Architecture: 1) Multi-modal classifiers (text, image, video); 2) Cascading system - fast cheap filters first, then expensive models; 3) Confidence thresholds - auto-action for high confidence, human review for uncertain; 4) Appeal handling and feedback loop; 5) Real-time and batch processing. Considerations: false positive impact on users, adversarial content, multilingual support, edge cases.",
    tips: "Discuss the human-in-the-loop aspect and how to handle appeals and errors.",
  },
  {
    id: "sd-6",
    question: "Design an ML system for dynamic pricing.",
    category: "System Design",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "Data Scientist"],
    stages: ["System Design"],
    sampleAnswer: "Components: 1) Demand forecasting model (time series, features: seasonality, events, competitors); 2) Price elasticity estimation; 3) Optimisation layer (balancing revenue, inventory, fairness); 4) A/B testing framework; 5) Guardrails for price bounds and rate limiting. Considerations: legal constraints, customer perception, competitor response, real-time vs batch updates.",
    tips: "Address ethical considerations around price discrimination and fairness.",
  },
  {
    id: "sd-7",
    question: "Design a search ranking system for an e-commerce platform.",
    category: "System Design",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "Data Scientist"],
    stages: ["System Design"],
    sampleAnswer: "Stages: 1) Query understanding (intent, spelling correction, expansion); 2) Candidate retrieval (inverted index, embedding search); 3) Ranking model (learning to rank with features: relevance, popularity, personalisation, freshness); 4) Re-ranking for diversity and business rules; 5) Real-time personalisation. Metrics: NDCG, MRR for relevance; CTR, conversion for business.",
    tips: "Discuss cold start for new products and users, and position bias in training data.",
  },
  {
    id: "sd-8",
    question: "How would you design an anomaly detection system for a large-scale distributed system?",
    category: "System Design",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "Data Engineer", "MLOps Engineer"],
    stages: ["System Design"],
    sampleAnswer: "Architecture: 1) Data collection from multiple sources (logs, metrics, traces); 2) Feature engineering for time series; 3) Multi-level detection (per-service, cluster-wide, cross-service); 4) Models: statistical methods for univariate, ML for multivariate; 5) Alert aggregation and deduplication; 6) Root cause analysis integration. Challenges: high dimensionality, concept drift, alert fatigue.",
    tips: "Discuss how to handle seasonality and how to reduce false positives.",
  },
  {
    id: "sd-9",
    question: "Design an ML pipeline for a company processing millions of images daily.",
    category: "System Design",
    difficulty: "Lead",
    roles: ["Machine Learning Engineer", "MLOps Engineer", "Data Engineer"],
    stages: ["System Design", "Final Round"],
    sampleAnswer: "Architecture: 1) Ingestion via message queue (Kafka); 2) Distributed processing (Spark/Ray); 3) Model serving with GPU clusters and auto-scaling; 4) Caching for repeated images; 5) Results storage (hot/warm/cold tiers); 6) Monitoring and alerting. Considerations: cost optimisation, latency vs throughput trade-offs, handling failures, model versioning, A/B testing infrastructure.",
    tips: "Calculate rough cost estimates and throughput numbers to show practical understanding.",
  },

  // Additional Coding & Algorithms
  {
    id: "code-5",
    question: "Implement a function to compute TF-IDF scores for a document corpus.",
    category: "Coding & Algorithms",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "Data Scientist"],
    stages: ["Technical Interview"],
    sampleAnswer: "TF (term frequency) = count of term in doc / total terms in doc. IDF (inverse document frequency) = log(total docs / docs containing term). TF-IDF = TF * IDF. Implementation: 1) Build vocabulary; 2) Compute TF for each doc; 3) Compute IDF across corpus; 4) Multiply to get TF-IDF matrix. Consider: smoothing for IDF, normalisation, sparse matrix representation.",
    tips: "Discuss scikit-learn's TfidfVectorizer and when you'd use it vs custom implementation.",
  },
  {
    id: "code-6",
    question: "Write code to implement softmax function and its numerical stability considerations.",
    category: "Coding & Algorithms",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "AI Researcher"],
    stages: ["Technical Interview"],
    sampleAnswer: "Softmax: exp(x_i) / sum(exp(x_j)). Numerical issue: exp of large numbers overflows. Solution: subtract max(x) from all values before exp - mathematically equivalent but stable. Implementation: shifted = x - max(x), exp_shifted = exp(shifted), return exp_shifted / sum(exp_shifted).",
    tips: "Always mention numerical stability - it shows attention to practical implementation details.",
  },
  {
    id: "code-7",
    question: "Implement k-means clustering algorithm from scratch.",
    category: "Coding & Algorithms",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "Data Scientist"],
    stages: ["Technical Interview"],
    sampleAnswer: "Steps: 1) Initialise k centroids (random or k-means++); 2) Assign each point to nearest centroid; 3) Update centroids as mean of assigned points; 4) Repeat until convergence (centroids don't change) or max iterations. K-means++ initialisation: choose first centroid randomly, then each subsequent centroid with probability proportional to squared distance from nearest existing centroid.",
    tips: "Discuss initialisation sensitivity and how k-means++ addresses it.",
  },
  {
    id: "code-8",
    question: "Write a function to detect if a dataset has data leakage.",
    category: "Coding & Algorithms",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "Data Scientist"],
    stages: ["Technical Interview"],
    sampleAnswer: "Checks: 1) Features perfectly correlated with target; 2) Features from the future (timestamp analysis); 3) Features derived from test set; 4) Duplicate or near-duplicate rows across train/test; 5) Unrealistically high performance on validation. Implementation: correlation analysis, temporal ordering checks, feature importance analysis, performance sanity checks.",
    tips: "Share a real example where you caught data leakage and how you identified it.",
  },
  {
    id: "code-9",
    question: "Implement logistic regression with gradient descent from scratch.",
    category: "Coding & Algorithms",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher"],
    stages: ["Technical Interview"],
    sampleAnswer: "Components: 1) Sigmoid function: 1/(1+exp(-z)); 2) Forward pass: predictions = sigmoid(X @ w + b); 3) Binary cross-entropy loss: -mean(y*log(p) + (1-y)*log(1-p)); 4) Gradients: dw = X.T @ (predictions - y) / n, db = mean(predictions - y); 5) Update: w -= lr * dw, b -= lr * db. Add regularisation term to loss and gradients.",
    tips: "Discuss how you'd extend this to multi-class (softmax) and add regularisation.",
  },
  {
    id: "code-10",
    question: "Implement a simple decision tree classifier.",
    category: "Coding & Algorithms",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "Data Scientist"],
    stages: ["Technical Interview"],
    sampleAnswer: "Algorithm: 1) Calculate impurity (Gini or entropy) for current node; 2) For each feature and threshold, calculate information gain; 3) Split on best feature/threshold; 4) Recursively build left and right subtrees; 5) Stop when max depth reached or node is pure. Prediction: traverse tree based on feature values until leaf, return majority class.",
    tips: "Discuss pruning strategies and how to handle continuous vs categorical features.",
  },

  // Additional Statistics & Math
  {
    id: "stats-5",
    question: "Explain p-values and statistical significance. What are common misconceptions?",
    category: "Statistics & Math",
    difficulty: "Junior",
    roles: ["Data Scientist", "AI Researcher"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "P-value is the probability of observing results as extreme as the data, assuming null hypothesis is true. Common misconceptions: 1) P-value is NOT the probability that the null hypothesis is true; 2) Statistical significance doesn't mean practical significance; 3) P < 0.05 is arbitrary; 4) Non-significant doesn't mean no effect. Always report effect sizes and confidence intervals alongside p-values.",
    tips: "Be prepared to critique p-value misuse in published studies.",
  },
  {
    id: "stats-6",
    question: "What is the difference between Type I and Type II errors?",
    category: "Statistics & Math",
    difficulty: "Junior",
    roles: ["Data Scientist", "AI Researcher"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "Type I (False Positive): rejecting null hypothesis when it's true. Probability = alpha (significance level). Type II (False Negative): failing to reject null when it's false. Probability = beta. Power = 1 - beta. Trade-off: reducing Type I increases Type II and vice versa. Context matters: medical screening prioritises reducing Type II (missing disease is worse than false alarm).",
    tips: "Give examples from ML: false positive in fraud detection vs false negative in disease diagnosis.",
  },
  {
    id: "stats-7",
    question: "Explain confidence intervals and how they differ from prediction intervals.",
    category: "Statistics & Math",
    difficulty: "Mid",
    roles: ["Data Scientist", "AI Researcher"],
    stages: ["Technical Interview"],
    sampleAnswer: "Confidence interval: range likely to contain the true population parameter (e.g., mean). Interpretation: if we repeated the experiment many times, 95% of CIs would contain the true parameter. Prediction interval: range likely to contain a future individual observation - always wider because it includes both parameter uncertainty and individual variation.",
    tips: "Clarify that a CI is about the parameter, not about where future data will fall.",
  },
  {
    id: "stats-8",
    question: "What is the difference between covariance and correlation?",
    category: "Statistics & Math",
    difficulty: "Junior",
    roles: ["Data Scientist", "AI Researcher", "Data Engineer"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "Covariance measures how two variables change together - positive means they increase together, negative means one increases as other decreases. Units are product of variable units, so hard to interpret. Correlation (Pearson's r) is standardised covariance, ranging from -1 to 1, making it comparable across different scales. Both only measure linear relationships.",
    tips: "Mention Spearman correlation for non-linear monotonic relationships.",
  },
  {
    id: "stats-9",
    question: "Explain the assumptions of linear regression and what happens when they're violated.",
    category: "Statistics & Math",
    difficulty: "Mid",
    roles: ["Data Scientist", "AI Researcher"],
    stages: ["Technical Interview"],
    sampleAnswer: "Assumptions: 1) Linearity - use residual plots, transform variables; 2) Independence of errors - check for autocorrelation; 3) Homoscedasticity (constant variance) - use weighted least squares; 4) Normality of errors - less critical for large samples; 5) No multicollinearity - remove or combine correlated features. Violations can lead to biased estimates, incorrect standard errors, or inefficient estimates.",
    tips: "Know how to diagnose each assumption and the remedies for violations.",
  },
  {
    id: "stats-10",
    question: "What is the difference between population and sample statistics?",
    category: "Statistics & Math",
    difficulty: "Junior",
    roles: ["Data Scientist", "AI Researcher"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "Population statistics describe the entire population (usually unknown). Sample statistics estimate population parameters from a subset. Example: population mean () vs sample mean (x). Sample statistics have sampling distributions - the sample mean's standard error is /n. As sample size increases, sample statistics converge to population parameters (law of large numbers).",
    tips: "Explain why we divide by n-1 for sample variance (Bessel's correction).",
  },

  // Additional MLOps & Deployment
  {
    id: "mlops-7",
    question: "What is data drift and how do you detect and handle it?",
    category: "MLOps & Deployment",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "MLOps Engineer", "Data Scientist"],
    stages: ["Technical Interview"],
    sampleAnswer: "Data drift is when input data distribution changes over time, causing model performance degradation. Detection: statistical tests (KS test, chi-squared), PSI (Population Stability Index), monitoring feature distributions. Handling: 1) Retrain on recent data; 2) Use online learning; 3) Ensemble with drift-aware weighting; 4) Feature engineering for robustness. Set up automated alerts for drift detection.",
    tips: "Distinguish between data drift, concept drift, and label drift.",
  },
  {
    id: "mlops-8",
    question: "Explain the difference between blue-green and canary deployments for ML models.",
    category: "MLOps & Deployment",
    difficulty: "Mid",
    roles: ["MLOps Engineer", "Machine Learning Engineer"],
    stages: ["Technical Interview"],
    sampleAnswer: "Blue-green: two identical environments, switch traffic completely from old (blue) to new (green). Quick rollback but requires double resources. Canary: gradually shift traffic (e.g., 1% -> 10% -> 50% -> 100%) while monitoring metrics. Safer for ML because you can detect issues with real traffic. Shadow mode: new model runs in parallel without affecting production, good for initial validation.",
    tips: "Discuss what metrics you'd monitor during a canary rollout for an ML model.",
  },
  {
    id: "mlops-9",
    question: "How do you ensure reproducibility in ML experiments?",
    category: "MLOps & Deployment",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "MLOps Engineer", "Data Scientist", "AI Engineer"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "Key practices: 1) Version control code, data, and configurations; 2) Set random seeds for all sources of randomness; 3) Pin dependency versions; 4) Log all hyperparameters and metrics; 5) Use containerisation (Docker); 6) Track data lineage; 7) Use experiment tracking tools (MLflow, W&B). Document environment details and hardware specifications.",
    tips: "Mention tools you've used and share a time when reproducibility saved you.",
  },
  {
    id: "mlops-10",
    question: "What are the key considerations when containerising ML models?",
    category: "MLOps & Deployment",
    difficulty: "Mid",
    roles: ["MLOps Engineer", "Machine Learning Engineer"],
    stages: ["Technical Interview"],
    sampleAnswer: "Considerations: 1) Image size - use multi-stage builds, slim base images; 2) Dependencies - pin versions, use requirements.txt/conda; 3) Model loading - bake in model or load at runtime; 4) GPU support - use NVIDIA base images; 5) Security - non-root user, scan for vulnerabilities; 6) Health checks and graceful shutdown; 7) Resource limits for memory and CPU.",
    tips: "Discuss the trade-off between image size and build time.",
  },
  {
    id: "mlops-11",
    question: "Describe how you would implement model explainability in production.",
    category: "MLOps & Deployment",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "MLOps Engineer", "Data Scientist"],
    stages: ["Technical Interview", "System Design"],
    sampleAnswer: "Approaches: 1) Global explanations - feature importance, partial dependence plots; 2) Local explanations - SHAP, LIME for individual predictions; 3) Logging explanations with predictions for audit; 4) Caching expensive explanations; 5) Different levels of detail for different users. Consider: latency impact, storage costs, regulatory requirements (GDPR right to explanation).",
    tips: "Discuss trade-offs between explanation fidelity and computational cost.",
  },

  // Additional Domain Specific
  {
    id: "domain-4",
    question: "How would you build a time series forecasting system for demand prediction?",
    category: "Domain Specific",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "Data Scientist"],
    stages: ["Technical Interview", "System Design"],
    sampleAnswer: "Approach: 1) Data analysis - seasonality, trend, stationarity tests; 2) Feature engineering - lags, rolling statistics, calendar features, external data (weather, events); 3) Model selection - statistical (ARIMA, Prophet) vs ML (XGBoost, LSTM); 4) Evaluation - proper time series CV, metrics (MAPE, RMSE); 5) Multiple horizons handling; 6) Uncertainty quantification. Consider hierarchical forecasting for multiple products/locations.",
    tips: "Discuss how to handle cold start for new products and seasonality of different lengths.",
  },
  {
    id: "domain-5",
    question: "Explain how you would build a sentiment analysis system for customer reviews.",
    category: "Domain Specific",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher"],
    stages: ["Technical Interview"],
    sampleAnswer: "Approach: 1) Data collection and labelling; 2) Preprocessing - tokenisation, handling negations, emojis; 3) Model options - lexicon-based (simple), traditional ML (TF-IDF + classifier), deep learning (fine-tuned BERT); 4) Handle domain-specific vocabulary and sarcasm; 5) Aspect-based sentiment for detailed analysis; 6) Continuous learning from user feedback.",
    tips: "Discuss challenges like sarcasm, domain-specific language, and multilingual reviews.",
  },
  {
    id: "domain-6",
    question: "How would you approach building a recommendation system for a streaming platform?",
    category: "Domain Specific",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "Data Scientist"],
    stages: ["Technical Interview", "System Design"],
    sampleAnswer: "Hybrid approach: 1) Collaborative filtering - user-item interactions, matrix factorisation; 2) Content-based - item features (genre, actors, descriptions); 3) Sequential recommendations - model viewing patterns; 4) Contextual - time of day, device, mood; 5) Diversity and serendipity; 6) Cold start handling - content-based for new items, popular items for new users. Optimise for engagement but consider binge-watching concerns.",
    tips: "Discuss the explore-exploit trade-off and how to handle filter bubbles.",
  },
  {
    id: "domain-7",
    question: "How would you build an ML system for predicting employee attrition?",
    category: "Domain Specific",
    difficulty: "Mid",
    roles: ["Data Scientist", "Machine Learning Engineer"],
    stages: ["Technical Interview"],
    sampleAnswer: "Approach: 1) Feature engineering - tenure, performance, compensation, engagement surveys, team dynamics; 2) Handle class imbalance; 3) Model selection - interpretable models often preferred (logistic regression, decision trees); 4) Survival analysis for time-to-event; 5) Feature importance for actionable insights; 6) Ethical considerations - avoid protected attributes, ensure fairness. Focus on actionable predictions that HR can act on.",
    tips: "Discuss privacy concerns and how to present results without enabling discrimination.",
  },
  {
    id: "domain-8",
    question: "How would you implement a document classification system for legal documents?",
    category: "Domain Specific",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher"],
    stages: ["Technical Interview", "System Design"],
    sampleAnswer: "Approach: 1) Domain understanding - work with legal experts on taxonomy; 2) Long document handling - chunking, hierarchical models, Longformer; 3) Fine-tune legal-specific models (Legal-BERT); 4) Multi-label classification if documents span categories; 5) Confidence thresholds for human review; 6) Explainability for legal compliance; 7) Handle document structure (sections, clauses). Active learning for efficient labelling.",
    tips: "Emphasise the importance of explainability and human oversight in legal applications.",
  },
  {
    id: "domain-9",
    question: "How would you build a named entity recognition system for medical records?",
    category: "Domain Specific",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "AI Researcher"],
    stages: ["Technical Interview"],
    sampleAnswer: "Approach: 1) Use domain-specific models (BioBERT, ClinicalBERT); 2) Entity types: medications, diseases, procedures, anatomical terms; 3) Handle abbreviations and misspellings; 4) Relation extraction between entities; 5) Negation and uncertainty detection; 6) Privacy-preserving training (federated learning, differential privacy); 7) Integration with medical ontologies (SNOMED, ICD). Strict validation with medical experts.",
    tips: "Discuss HIPAA compliance and the importance of high precision in medical applications.",
  },

  // Additional Behavioral
  {
    id: "behav-6",
    question: "Describe a project where you had to make a trade-off between model complexity and interpretability.",
    category: "Behavioral",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher"],
    stages: ["Behavioral", "Final Round"],
    sampleAnswer: "Use STAR: describe the business context requiring interpretability, the trade-off analysis you performed (comparing model performance vs explainability), the decision made (perhaps using a simpler model or adding explanation layers to a complex one), and the outcome. Emphasise stakeholder communication and how interpretability requirements were balanced with performance needs.",
    tips: "Have specific metrics to show the performance gap you accepted for interpretability.",
  },
  {
    id: "behav-7",
    question: "Tell me about a time when you had to learn a new technology quickly for a project.",
    category: "Behavioral",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher", "Data Engineer", "MLOps Engineer", "AI Engineer", "AI Governance Specialist", "Software Developer - AI"],
    stages: ["Behavioral"],
    sampleAnswer: "Structure: 1) What was the technology and why it was needed; 2) How you approached learning (documentation, courses, hands-on projects); 3) Resources and timeline; 4) How you applied it to the project; 5) Outcome and what you learned about learning. Show self-direction and ability to become productive quickly.",
    tips: "Emphasise your learning process and how you'd approach similar situations in the future.",
  },
  {
    id: "behav-8",
    question: "How do you handle situations where stakeholders have unrealistic expectations about ML capabilities?",
    category: "Behavioral",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher", "MLOps Engineer"],
    stages: ["Behavioral", "Final Round"],
    sampleAnswer: "Approach: 1) Listen to understand their goals and assumptions; 2) Educate about ML capabilities and limitations using concrete examples; 3) Propose achievable milestones that build toward their vision; 4) Use proof-of-concepts to demonstrate what's possible; 5) Set clear success criteria and manage expectations throughout. Balance being helpful with being realistic.",
    tips: "Share a specific example where you reset expectations successfully.",
  },
  {
    id: "behav-9",
    question: "Describe how you approach working with cross-functional teams (engineering, product, business).",
    category: "Behavioral",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "Data Scientist", "MLOps Engineer"],
    stages: ["Behavioral"],
    sampleAnswer: "Principles: 1) Understand each team's objectives and constraints; 2) Translate technical concepts for non-technical stakeholders; 3) Align ML metrics with business metrics; 4) Regular communication and demos; 5) Document decisions and trade-offs; 6) Be flexible on implementation details while firm on fundamental requirements. Build relationships beyond just project needs.",
    tips: "Give examples of how you've adapted your communication style for different audiences.",
  },
  {
    id: "behav-10",
    question: "Tell me about a time you had to deliver difficult news about a project (delays, performance issues, etc.).",
    category: "Behavioral",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher", "MLOps Engineer"],
    stages: ["Behavioral", "Final Round"],
    sampleAnswer: "Key elements: 1) Early communication rather than waiting; 2) Come with analysis of what went wrong; 3) Present options and recommendations; 4) Take responsibility without making excuses; 5) Have a plan to address the issue; 6) Learn and implement preventive measures. Show maturity in handling setbacks.",
    tips: "Emphasise proactive communication and how you maintained trust despite the setback.",
  },

  // Additional Case Study
  {
    id: "case-5",
    question: "Your model is showing high accuracy in offline evaluation but poor performance in production. How would you investigate?",
    category: "Case Study",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "Data Scientist", "MLOps Engineer"],
    stages: ["Technical Interview"],
    sampleAnswer: "Investigation: 1) Check for data drift between training and production; 2) Verify feature computation consistency (training-serving skew); 3) Analyse performance by segment - might be failing for specific subgroups; 4) Check for label leakage in training; 5) Verify offline evaluation methodology (proper temporal splits?); 6) Review preprocessing differences; 7) Check for latency issues affecting features. Systematic debugging with logging.",
    tips: "Walk through each hypothesis methodically and explain how you'd validate each.",
  },
  {
    id: "case-6",
    question: "A stakeholder wants to use ML to automate a decision that's currently made by humans. How would you approach this?",
    category: "Case Study",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "Data Scientist"],
    stages: ["Behavioral", "Final Round"],
    sampleAnswer: "Framework: 1) Understand the current process and decision criteria; 2) Assess data availability and quality; 3) Define success metrics aligned with business goals; 4) Consider partial automation (human-in-the-loop) vs full automation; 5) Evaluate risks of errors and need for explainability; 6) Plan for edge cases and appeals; 7) Consider ethical implications. Start with augmenting human decisions rather than replacing them.",
    tips: "Discuss the change management aspect and how to get human experts on board.",
  },
  {
    id: "case-7",
    question: "How would you approach building an ML solution with very limited historical data?",
    category: "Case Study",
    difficulty: "Senior",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher"],
    stages: ["Technical Interview", "Final Round"],
    sampleAnswer: "Strategies: 1) Transfer learning from related domains; 2) Data augmentation techniques; 3) Synthetic data generation; 4) Active learning to efficiently label most informative samples; 5) Few-shot learning approaches; 6) Incorporate domain knowledge via feature engineering or priors; 7) Use simpler models that need less data; 8) Collect more data strategically. Combine multiple approaches based on domain.",
    tips: "Quantify what 'limited' means and provide specific thresholds for different strategies.",
  },
  {
    id: "case-8",
    question: "Your company wants to implement an AI chatbot for customer service. How would you design this project?",
    category: "Case Study",
    difficulty: "Mid",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher"],
    stages: ["System Design", "Final Round"],
    sampleAnswer: "Design: 1) Define scope - which queries to handle automatically; 2) Intent classification and entity extraction; 3) Retrieval-based vs generative responses (RAG); 4) Escalation to human agents for complex/sensitive issues; 5) Conversation management and context tracking; 6) Integration with existing systems (CRM, knowledge base); 7) Continuous learning from interactions; 8) Metrics: resolution rate, customer satisfaction, escalation rate.",
    tips: "Discuss handling edge cases, frustrated customers, and sensitive topics.",
  },

  // More Lead-level questions
  {
    id: "lead-4",
    question: "How do you balance exploration (trying new approaches) vs exploitation (improving existing systems)?",
    category: "Behavioral",
    difficulty: "Lead",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher", "MLOps Engineer"],
    stages: ["Final Round"],
    sampleAnswer: "Framework: 1) Dedicated time allocation (e.g., 70% production, 20% improvements, 10% exploration); 2) Clear criteria for when to invest in new approaches; 3) Proof-of-concept gates before full investment; 4) Track technical debt and improvement opportunities; 5) Balance team interests with business needs; 6) Create safe spaces for experimentation. Document learnings even from failed experiments.",
    tips: "Give specific examples of how you've managed this trade-off.",
  },
  {
    id: "lead-5",
    question: "How would you define and measure the ROI of an ML team?",
    category: "Case Study",
    difficulty: "Lead",
    roles: ["Machine Learning Engineer", "Data Scientist", "MLOps Engineer"],
    stages: ["Final Round"],
    sampleAnswer: "Metrics: 1) Direct value - revenue increase, cost reduction from ML solutions; 2) Efficiency gains - automation, faster decisions; 3) Quality improvements - error reduction, customer satisfaction; 4) Strategic value - competitive advantage, new capabilities. Measurement: A/B tests for incremental impact, counterfactual analysis. Challenges: attribution, long-term vs short-term value, platform/infrastructure investments.",
    tips: "Discuss how you'd communicate ML value to non-technical executives.",
  },
  {
    id: "lead-6",
    question: "How do you establish ML best practices and governance in an organisation?",
    category: "MLOps & Deployment",
    difficulty: "Lead",
    roles: ["Machine Learning Engineer", "MLOps Engineer"],
    stages: ["Final Round"],
    sampleAnswer: "Approach: 1) Start with documentation of existing practices; 2) Identify gaps and pain points; 3) Establish core standards (code review, testing, documentation, model validation); 4) Create templates and tooling to make compliance easy; 5) Regular audits and retrospectives; 6) Training and enablement; 7) Balance standardisation with flexibility for innovation. Get buy-in by solving real problems, not imposing bureaucracy.",
    tips: "Share examples of specific guidelines you've established and how you got adoption.",
  },
  {
    id: "lead-7",
    question: "How do you handle ethical concerns in ML projects?",
    category: "Behavioral",
    difficulty: "Lead",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher", "MLOps Engineer"],
    stages: ["Behavioral", "Final Round"],
    sampleAnswer: "Framework: 1) Proactive identification of potential harms; 2) Fairness analysis across demographic groups; 3) Privacy impact assessment; 4) Transparency in how models are used; 5) Clear escalation path for concerns; 6) Regular bias audits; 7) Stakeholder engagement including affected communities. Create space for team members to raise concerns without fear. Document ethical considerations in project planning.",
    tips: "Give a specific example of an ethical concern you identified and how you addressed it.",
  },
  {
    id: "lead-8",
    question: "Describe your approach to mentoring and developing junior ML engineers.",
    category: "Behavioral",
    difficulty: "Lead",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher", "MLOps Engineer"],
    stages: ["Behavioral", "Final Round"],
    sampleAnswer: "Approach: 1) Understand individual goals and learning styles; 2) Provide challenging but achievable projects; 3) Regular 1:1s for feedback and guidance; 4) Pair programming and code review as teaching moments; 5) Encourage conference talks, blog posts, open source; 6) Create psychological safety for asking questions; 7) Gradually increase autonomy while remaining available. Track growth and celebrate wins.",
    tips: "Share specific examples of people you've mentored and their growth.",
  },

  // More Data Engineer specific
  {
    id: "de-1",
    question: "Explain the differences between data lakes and data warehouses. When would you use each?",
    category: "System Design",
    difficulty: "Junior",
    roles: ["Data Engineer", "MLOps Engineer", "Data Scientist"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "Data warehouse: structured, schema-on-write, optimised for BI/analytics queries, typically uses SQL. Data lake: raw data in various formats, schema-on-read, cheaper storage, flexible for ML and exploration. Use warehouse for business reporting and known queries; use lake for ML training data, exploration, and storing raw data for future use. Modern: lakehouse combines both (Delta Lake, Iceberg).",
    tips: "Mention specific technologies you've used: Snowflake, BigQuery, S3, Databricks.",
  },
  {
    id: "de-2",
    question: "How do you ensure data quality in a data pipeline?",
    category: "MLOps & Deployment",
    difficulty: "Mid",
    roles: ["Data Engineer", "MLOps Engineer"],
    stages: ["Technical Interview"],
    sampleAnswer: "Practices: 1) Data validation at ingestion (schema, types, ranges); 2) Automated quality checks (completeness, uniqueness, consistency); 3) Data profiling and monitoring; 4) Anomaly detection for data drift; 5) Data contracts between producers and consumers; 6) Lineage tracking; 7) Alerting on quality issues. Tools: Great Expectations, dbt tests, Soda. Build quality checks into CI/CD.",
    tips: "Discuss specific data quality issues you've encountered and how you addressed them.",
  },
  {
    id: "de-3",
    question: "What is the difference between batch and stream processing? How do you choose between them?",
    category: "System Design",
    difficulty: "Mid",
    roles: ["Data Engineer", "MLOps Engineer", "Machine Learning Engineer"],
    stages: ["Technical Interview"],
    sampleAnswer: "Batch: processes data in chunks periodically, higher throughput, simpler to implement. Streaming: processes data continuously with low latency, more complex, handles late/out-of-order data. Choose based on: latency requirements, data volume, complexity of processing, cost. Many systems use both: streaming for real-time features, batch for historical analysis. Tools: batch (Spark), streaming (Kafka, Flink).",
    tips: "Give examples of ML features that need real-time vs batch processing.",
  },
  {
    id: "de-4",
    question: "How would you design a data pipeline to support ML model training at scale?",
    category: "System Design",
    difficulty: "Senior",
    roles: ["Data Engineer", "MLOps Engineer"],
    stages: ["System Design"],
    sampleAnswer: "Components: 1) Data ingestion from multiple sources; 2) Data validation and quality checks; 3) Feature computation (batch and streaming); 4) Feature store for serving and training; 5) Data versioning for reproducibility; 6) Point-in-time correct joins to prevent leakage; 7) Efficient storage formats (Parquet, TFRecord); 8) Scalable compute (Spark, Ray). Consider: backfilling, schema evolution, cost optimisation.",
    tips: "Discuss how you'd handle late-arriving data and schema changes.",
  },

  // Final additions to ensure coverage
  {
    id: "extra-1",
    question: "What are word embeddings and why are they useful in NLP?",
    category: "ML Fundamentals",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher", "AI Engineer", "Software Developer - AI"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "Word embeddings are dense vector representations of words that capture semantic meaning. Similar words have similar vectors. Created by models like Word2Vec (skip-gram, CBOW), GloVe, or as part of larger models (BERT). Useful because: enable mathematical operations on words (king - man + woman  queen), reduce dimensionality vs one-hot encoding, transfer learning across tasks.",
    tips: "Be ready to explain the difference between static (Word2Vec) and contextual (BERT) embeddings.",
  },
  {
    id: "extra-2",
    question: "How do you handle categorical variables in machine learning?",
    category: "ML Fundamentals",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "Data Scientist", "Data Engineer"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "Methods: 1) One-hot encoding - creates binary columns, good for low cardinality; 2) Label encoding - assigns integers, use with tree-based models; 3) Target encoding - encodes with target statistics, handles high cardinality; 4) Embedding layers - learns representations in neural networks; 5) Binary encoding - for high cardinality. Consider cardinality, model type, and whether order matters.",
    tips: "Discuss how to prevent target leakage when using target encoding.",
  },
  {
    id: "extra-3",
    question: "Explain precision vs recall vs F1 score with examples.",
    category: "ML Fundamentals",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "Data Scientist"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "Precision: of predicted positives, how many are correct (TP/(TP+FP)). High precision = few false alarms. Recall: of actual positives, how many did we find (TP/(TP+FN)). High recall = few missed cases. F1: harmonic mean of both. Example: spam filter - high precision avoids marking real email as spam; disease screening - high recall avoids missing sick patients. F1 when you need balance.",
    tips: "Discuss how you'd choose which metric to optimise based on business context.",
  },
  {
    id: "extra-4",
    question: "What is overfitting and how do you detect and prevent it?",
    category: "ML Fundamentals",
    difficulty: "Junior",
    roles: ["Machine Learning Engineer", "Data Scientist", "AI Researcher"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "Overfitting: model memorises training data instead of learning patterns, performs poorly on new data. Detection: large gap between training and validation performance, validation loss increasing while training loss decreases. Prevention: more data, regularisation (L1/L2, dropout), cross-validation, early stopping, simpler model, data augmentation, ensemble methods.",
    tips: "Draw learning curves to illustrate overfitting vs underfitting.",
  },

  // ==========================================
  // AI Engineer Questions
  // ==========================================
  {
    id: "aie-1",
    question: "What is the difference between an AI Engineer and a Machine Learning Engineer?",
    category: "ML Fundamentals",
    difficulty: "Junior",
    roles: ["AI Engineer"],
    stages: ["Phone Screen"],
    sampleAnswer: "AI Engineers focus on building and integrating AI-powered applications and products, often working with pre-trained models, APIs, and AI services. ML Engineers focus more on developing, training, and optimising custom ML models. AI Engineers typically work closer to the product/application layer, integrating LLMs, computer vision APIs, and other AI services into software systems.",
    tips: "Emphasise your experience building end-to-end AI applications, not just models.",
  },
  {
    id: "aie-2",
    question: "How would you integrate a large language model into an existing application?",
    category: "System Design",
    difficulty: "Mid",
    roles: ["AI Engineer", "Software Developer - AI"],
    stages: ["Technical Interview", "System Design"],
    sampleAnswer: "Key considerations: 1) API vs self-hosted model (cost, latency, data privacy); 2) Prompt engineering and management; 3) Context window limitations and chunking strategies; 4) Caching for repeated queries; 5) Rate limiting and error handling; 6) Output validation and guardrails; 7) Monitoring usage and costs; 8) Fallback strategies. Use async processing for long operations, implement streaming for better UX.",
    tips: "Discuss specific LLM APIs you've worked with (OpenAI, Anthropic, etc.) and their trade-offs.",
  },
  {
    id: "aie-3",
    question: "Explain prompt engineering. What techniques do you use to improve LLM outputs?",
    category: "Domain Specific",
    difficulty: "Mid",
    roles: ["AI Engineer", "Software Developer - AI", "AI Researcher"],
    stages: ["Technical Interview"],
    sampleAnswer: "Techniques: 1) Clear instructions with specific format requirements; 2) Few-shot examples showing desired output; 3) Chain-of-thought prompting for reasoning tasks; 4) System prompts for persona and constraints; 5) Breaking complex tasks into steps; 6) Output structuring (JSON mode); 7) Temperature tuning for creativity vs consistency. Iterate based on failure cases, maintain prompt versioning.",
    tips: "Share specific examples where prompt engineering significantly improved results.",
  },
  {
    id: "aie-4",
    question: "How do you evaluate the quality of LLM outputs in production?",
    category: "MLOps & Deployment",
    difficulty: "Mid",
    roles: ["AI Engineer", "Software Developer - AI"],
    stages: ["Technical Interview"],
    sampleAnswer: "Evaluation approaches: 1) Automated metrics - BLEU, ROUGE for text similarity, task-specific metrics; 2) LLM-as-judge for quality assessment; 3) Human evaluation with clear rubrics; 4) A/B testing with user engagement metrics; 5) Safety and toxicity classifiers; 6) Factuality checking against sources; 7) User feedback collection. Combine automated and human evaluation for comprehensive assessment.",
    tips: "Discuss how you'd handle subjective quality measures and edge cases.",
  },
  {
    id: "aie-5",
    question: "What is RAG (Retrieval-Augmented Generation) and when would you use it?",
    category: "System Design",
    difficulty: "Mid",
    roles: ["AI Engineer", "Machine Learning Engineer", "Software Developer - AI"],
    stages: ["Technical Interview", "System Design"],
    sampleAnswer: "RAG combines retrieval systems with generative models. Process: 1) Index documents into vector database; 2) Convert user query to embedding; 3) Retrieve relevant documents; 4) Include retrieved context in LLM prompt; 5) Generate response grounded in retrieved information. Use when: knowledge needs to be current, domain-specific, or verifiable. Benefits: reduces hallucinations, enables citation, updates without retraining.",
    tips: "Discuss chunking strategies, embedding models, and reranking approaches.",
  },
  {
    id: "aie-6",
    question: "How do you handle rate limits and costs when working with AI APIs?",
    category: "MLOps & Deployment",
    difficulty: "Junior",
    roles: ["AI Engineer", "Software Developer - AI"],
    stages: ["Technical Interview"],
    sampleAnswer: "Strategies: 1) Implement request queuing and rate limiting; 2) Cache responses for repeated queries; 3) Use smaller/cheaper models for simple tasks; 4) Batch requests where possible; 5) Implement exponential backoff for retries; 6) Monitor usage and set budget alerts; 7) Use tiered approach - cheap model first, expensive model for complex queries. Track cost per feature to inform product decisions.",
    tips: "Share specific cost optimisation wins from your experience.",
  },
  {
    id: "aie-7",
    question: "Describe how you would build an AI-powered chatbot for customer support.",
    category: "System Design",
    difficulty: "Senior",
    roles: ["AI Engineer", "Software Developer - AI"],
    stages: ["System Design", "Final Round"],
    sampleAnswer: "Architecture: 1) Intent classification to route queries; 2) RAG system with knowledge base; 3) Conversation memory management; 4) Guardrails for off-topic and harmful content; 5) Escalation to human agents; 6) Multi-turn context handling; 7) Integration with CRM and ticketing systems; 8) Analytics and feedback loop. Consider: latency, accuracy vs coverage trade-off, handling edge cases, multilingual support.",
    tips: "Discuss how you'd measure success beyond just accuracy - resolution rate, customer satisfaction.",
  },
  {
    id: "aie-8",
    question: "What are embeddings and how do you use them in AI applications?",
    category: "ML Fundamentals",
    difficulty: "Junior",
    roles: ["AI Engineer", "Software Developer - AI", "Machine Learning Engineer"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "Embeddings are dense vector representations that capture semantic meaning. Uses: 1) Semantic search - find similar items; 2) RAG systems - retrieve relevant documents; 3) Recommendation systems - user/item similarity; 4) Clustering and classification; 5) Deduplication. Choose embedding model based on domain, use cosine similarity for comparison, store in vector databases (Pinecone, Weaviate, pgvector).",
    tips: "Mention specific embedding models you've used and their trade-offs.",
  },
  {
    id: "aie-9",
    question: "How do you handle hallucinations in LLM applications?",
    category: "Domain Specific",
    difficulty: "Mid",
    roles: ["AI Engineer", "Software Developer - AI", "AI Researcher"],
    stages: ["Technical Interview"],
    sampleAnswer: "Mitigation strategies: 1) RAG to ground responses in facts; 2) Constrain outputs to known options; 3) Ask model to cite sources; 4) Implement fact-checking layer; 5) Use lower temperature for factual tasks; 6) Fine-tune on domain-specific data; 7) Add disclaimers for uncertain outputs; 8) Human review for high-stakes decisions. Detection: cross-reference with knowledge base, consistency checks, confidence scoring.",
    tips: "Discuss the trade-off between helpfulness and accuracy in production systems.",
  },
  {
    id: "aie-10",
    question: "Explain vector databases. When would you use one and how do you choose?",
    category: "System Design",
    difficulty: "Mid",
    roles: ["AI Engineer", "Software Developer - AI", "Data Engineer"],
    stages: ["Technical Interview"],
    sampleAnswer: "Vector databases store and efficiently query high-dimensional vectors using approximate nearest neighbour (ANN) algorithms. Use for: semantic search, RAG, recommendations, image similarity. Choosing: 1) Scale requirements; 2) Latency needs; 3) Filtering capabilities; 4) Managed vs self-hosted; 5) Cost; 6) Integration with existing stack. Options: Pinecone (managed), Weaviate (open source), pgvector (PostgreSQL extension), Chroma (lightweight).",
    tips: "Discuss indexing strategies (HNSW, IVF) and their trade-offs.",
  },

  // ==========================================
  // AI Governance Specialist Questions
  // ==========================================
  {
    id: "aig-1",
    question: "What is AI governance and why is it important?",
    category: "Domain Specific",
    difficulty: "Junior",
    roles: ["AI Governance Specialist"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "AI governance is the framework of policies, processes, and practices that ensure AI systems are developed and used responsibly. It covers: ethics, fairness, transparency, accountability, privacy, and compliance. Important because: AI can cause harm at scale, regulatory requirements are increasing, public trust is essential, and organisations face reputational and legal risks from AI failures.",
    tips: "Reference specific regulations like EU AI Act, Australian AI Ethics Framework.",
  },
  {
    id: "aig-2",
    question: "Explain the concept of algorithmic bias. How do you identify and mitigate it?",
    category: "Domain Specific",
    difficulty: "Mid",
    roles: ["AI Governance Specialist", "Data Scientist", "AI Researcher"],
    stages: ["Technical Interview", "Behavioral"],
    sampleAnswer: "Algorithmic bias occurs when AI systems produce unfair outcomes for certain groups. Sources: biased training data, biased labels, proxy variables, feedback loops. Identification: disaggregated performance metrics across groups, fairness metrics (demographic parity, equalised odds), audit tools. Mitigation: diverse training data, bias-aware sampling, fairness constraints in training, regular audits, diverse development teams.",
    tips: "Give examples of real-world AI bias cases and how they could have been prevented.",
  },
  {
    id: "aig-3",
    question: "What is explainable AI (XAI) and why does it matter for governance?",
    category: "Domain Specific",
    difficulty: "Mid",
    roles: ["AI Governance Specialist", "AI Researcher", "Data Scientist"],
    stages: ["Technical Interview"],
    sampleAnswer: "XAI provides human-understandable explanations for AI decisions. Methods: SHAP, LIME for feature importance; attention visualisation; counterfactual explanations; rule extraction. Governance importance: 1) Regulatory compliance (GDPR right to explanation); 2) Debugging and improving models; 3) Building user trust; 4) Detecting bias; 5) Accountability when things go wrong. Balance between model performance and interpretability.",
    tips: "Discuss different explanation needs for different stakeholders (users, auditors, developers).",
  },
  {
    id: "aig-4",
    question: "How would you implement an AI ethics review process for your organisation?",
    category: "Case Study",
    difficulty: "Senior",
    roles: ["AI Governance Specialist"],
    stages: ["Technical Interview", "Final Round"],
    sampleAnswer: "Framework: 1) Establish ethics principles aligned with company values; 2) Create risk assessment checklist for AI projects; 3) Form diverse ethics review board; 4) Define review triggers (high-risk use cases, sensitive data); 5) Document decisions and rationale; 6) Regular training for developers; 7) Feedback mechanism and incident response; 8) Periodic audits of deployed systems. Make it enabling, not just blocking.",
    tips: "Discuss how to balance innovation speed with responsible development.",
  },
  {
    id: "aig-5",
    question: "What are the key AI regulations globally and how do they differ?",
    category: "Domain Specific",
    difficulty: "Senior",
    roles: ["AI Governance Specialist"],
    stages: ["Technical Interview", "Final Round"],
    sampleAnswer: "Key regulations: 1) EU AI Act - risk-based approach, strict requirements for high-risk AI; 2) GDPR - data protection, automated decision-making rights; 3) Australian AI Ethics Principles - voluntary framework; 4) US - sector-specific (FDA for medical AI); 5) China - algorithm recommendation rules, deep synthesis rules. Key differences: mandatory vs voluntary, risk categorisation, enforcement mechanisms, scope of application.",
    tips: "Stay current with regulatory developments and their practical implications.",
  },
  {
    id: "aig-6",
    question: "How do you conduct a fairness audit of an ML model?",
    category: "Case Study",
    difficulty: "Mid",
    roles: ["AI Governance Specialist", "Data Scientist"],
    stages: ["Technical Interview"],
    sampleAnswer: "Audit process: 1) Define protected attributes and relevant fairness metrics; 2) Collect demographic data or use proxies carefully; 3) Calculate performance metrics disaggregated by group; 4) Apply statistical tests for significant differences; 5) Analyse feature importance for proxy discrimination; 6) Test with counterfactual examples; 7) Document findings and recommendations; 8) Establish ongoing monitoring. Consider intersectionality.",
    tips: "Discuss challenges like getting demographic data and defining appropriate fairness criteria.",
  },
  {
    id: "aig-7",
    question: "What is model documentation and what should it include?",
    category: "MLOps & Deployment",
    difficulty: "Junior",
    roles: ["AI Governance Specialist", "MLOps Engineer", "Data Scientist"],
    stages: ["Phone Screen", "Technical Interview"],
    sampleAnswer: "Model documentation (model cards) should include: 1) Model purpose and intended use; 2) Training data description and limitations; 3) Performance metrics across subgroups; 4) Ethical considerations and potential harms; 5) Limitations and out-of-scope uses; 6) Version history; 7) Maintenance and update plans; 8) Contact information. Enables transparency, accountability, and informed use.",
    tips: "Reference Google's Model Cards paper and examples from major AI companies.",
  },
  {
    id: "aig-8",
    question: "How do you balance AI innovation with responsible development?",
    category: "Behavioral",
    difficulty: "Senior",
    roles: ["AI Governance Specialist"],
    stages: ["Behavioral", "Final Round"],
    sampleAnswer: "Approach: 1) Embed governance early in development lifecycle, not as an afterthought; 2) Create clear guidelines that enable rather than block; 3) Provide tools and training to make compliance easy; 4) Risk-based approach - proportionate review for risk level; 5) Learn from incidents across industry; 6) Celebrate responsible innovation examples; 7) Engage with product teams as partners. Goal is sustainable innovation.",
    tips: "Give examples of how governance enabled better products, not just prevented harm.",
  },
  {
    id: "aig-9",
    question: "What privacy considerations are specific to AI systems?",
    category: "Domain Specific",
    difficulty: "Mid",
    roles: ["AI Governance Specialist", "Data Engineer"],
    stages: ["Technical Interview"],
    sampleAnswer: "AI-specific privacy concerns: 1) Training data may contain PII; 2) Models can memorise and leak training data; 3) Inference attacks can reveal training data; 4) Embeddings may encode sensitive information; 5) Prompt injection can extract system information; 6) LLMs may reveal conversation data. Mitigations: differential privacy, federated learning, data anonymisation, access controls, output filtering, regular privacy audits.",
    tips: "Discuss specific techniques like membership inference attacks and how to defend against them.",
  },
  {
    id: "aig-10",
    question: "How would you handle a situation where a deployed AI system is found to be causing harm?",
    category: "Case Study",
    difficulty: "Senior",
    roles: ["AI Governance Specialist"],
    stages: ["Behavioral", "Final Round"],
    sampleAnswer: "Response framework: 1) Immediate assessment of harm scope and severity; 2) Decision on whether to pause, roll back, or continue with guardrails; 3) Root cause analysis - data, model, or application issue; 4) Communication to affected parties and stakeholders; 5) Remediation plan with timeline; 6) Documentation for learning and accountability; 7) Process improvements to prevent recurrence; 8) Consider disclosure obligations.",
    tips: "Discuss how you'd balance transparency with legal considerations.",
  },
  {
    id: "aig-11",
    question: "What is responsible AI and how do you operationalise it?",
    category: "Domain Specific",
    difficulty: "Mid",
    roles: ["AI Governance Specialist", "AI Engineer"],
    stages: ["Technical Interview"],
    sampleAnswer: "Responsible AI principles: fairness, transparency, privacy, safety, accountability, human oversight. Operationalising: 1) Translate principles into measurable requirements; 2) Integrate into ML lifecycle (design, data, training, deployment, monitoring); 3) Create tooling for fairness testing, explainability; 4) Training and awareness programs; 5) Governance structure with clear roles; 6) Regular assessment and improvement. Make it part of definition of done.",
    tips: "Share examples of specific responsible AI practices you've implemented.",
  },

  // ==========================================
  // Software Developer - AI Questions
  // ==========================================
  {
    id: "sda-1",
    question: "How do you structure a codebase for an AI-powered application?",
    category: "System Design",
    difficulty: "Mid",
    roles: ["Software Developer - AI", "AI Engineer"],
    stages: ["Technical Interview"],
    sampleAnswer: "Structure: 1) Separate AI/ML components from application logic; 2) Abstract AI services behind interfaces for swapping providers; 3) Configuration-driven prompts and model selection; 4) Dedicated modules for: prompt management, response parsing, caching, rate limiting; 5) Clear error handling for AI failures; 6) Logging for debugging AI interactions; 7) Testing infrastructure including mock AI responses. Follow standard software engineering practices.",
    tips: "Discuss how you handle the non-deterministic nature of AI in your code structure.",
  },
  {
    id: "sda-2",
    question: "What testing strategies do you use for AI-integrated applications?",
    category: "Coding & Algorithms",
    difficulty: "Mid",
    roles: ["Software Developer - AI", "AI Engineer"],
    stages: ["Technical Interview"],
    sampleAnswer: "Testing layers: 1) Unit tests with mocked AI responses for deterministic behavior; 2) Integration tests with real AI calls (separate test suite, higher cost); 3) Evaluation sets for AI quality (golden datasets); 4) Snapshot testing for prompt changes; 5) Load testing for rate limits and latency; 6) Chaos testing for API failures. Use test fixtures with representative AI responses, version control your evaluation sets.",
    tips: "Discuss how you balance test coverage with API costs.",
  },
  {
    id: "sda-3",
    question: "How do you handle errors and edge cases when calling AI APIs?",
    category: "Coding & Algorithms",
    difficulty: "Junior",
    roles: ["Software Developer - AI", "AI Engineer"],
    stages: ["Technical Interview"],
    sampleAnswer: "Error handling: 1) Retry with exponential backoff for transient failures; 2) Circuit breaker pattern for persistent failures; 3) Fallback responses or graceful degradation; 4) Timeout handling for slow responses; 5) Rate limit detection and queuing; 6) Input validation before API calls; 7) Output validation and sanitisation; 8) Logging and alerting for debugging. Always have a user-friendly error message for AI failures.",
    tips: "Share specific error scenarios you've encountered and how you handled them.",
  },
  {
    id: "sda-4",
    question: "Explain how you would implement streaming responses from an LLM in a web application.",
    category: "Coding & Algorithms",
    difficulty: "Mid",
    roles: ["Software Developer - AI", "AI Engineer"],
    stages: ["Technical Interview"],
    sampleAnswer: "Implementation: 1) Use Server-Sent Events (SSE) or WebSockets for real-time updates; 2) Backend streams from LLM API and forwards chunks to frontend; 3) Frontend renders tokens as they arrive; 4) Handle connection drops and reconnection; 5) Implement cancel functionality; 6) Buffer and debounce for smooth rendering; 7) Handle markdown/code formatting progressively. Consider: error handling mid-stream, timeouts, mobile connections.",
    tips: "Discuss the UX benefits of streaming vs waiting for complete response.",
  },
  {
    id: "sda-5",
    question: "How do you manage prompts in a production application?",
    category: "MLOps & Deployment",
    difficulty: "Mid",
    roles: ["Software Developer - AI", "AI Engineer"],
    stages: ["Technical Interview"],
    sampleAnswer: "Prompt management: 1) Store prompts separately from code (config files, database); 2) Version control with change history; 3) Template system with variable substitution; 4) A/B testing infrastructure for prompt variants; 5) Evaluation pipeline to test changes; 6) Rollback capability; 7) Environment-specific prompts (dev/staging/prod); 8) Documentation of prompt purpose and expected behavior. Treat prompts like code with review processes.",
    tips: "Mention tools like LangChain, PromptLayer, or custom solutions you've built.",
  },
  {
    id: "sda-6",
    question: "What design patterns are useful for AI-powered applications?",
    category: "System Design",
    difficulty: "Senior",
    roles: ["Software Developer - AI", "AI Engineer"],
    stages: ["Technical Interview"],
    sampleAnswer: "Patterns: 1) Strategy pattern - swap AI providers/models; 2) Chain of Responsibility - multi-step AI pipelines; 3) Observer - react to AI events/streaming; 4) Facade - simplify complex AI integrations; 5) Retry/Circuit Breaker - handle failures; 6) Cache-Aside - store AI responses; 7) Router - direct queries to appropriate models; 8) Human-in-the-loop - escalation patterns. Apply standard patterns with AI-specific considerations.",
    tips: "Give concrete examples of how you've applied these patterns.",
  },
  {
    id: "sda-7",
    question: "How do you ensure security when building AI-powered applications?",
    category: "System Design",
    difficulty: "Senior",
    roles: ["Software Developer - AI", "AI Engineer", "AI Governance Specialist"],
    stages: ["Technical Interview", "System Design"],
    sampleAnswer: "Security considerations: 1) Prompt injection prevention - validate and sanitise inputs; 2) Output filtering for sensitive data; 3) API key management and rotation; 4) Rate limiting per user; 5) Audit logging of AI interactions; 6) Data privacy - don't send PII to external APIs; 7) Access controls for AI features; 8) Monitor for abuse patterns. Treat AI as untrusted - validate outputs before using in critical paths.",
    tips: "Discuss specific prompt injection attacks and defense strategies.",
  },
  {
    id: "sda-8",
    question: "How do you optimise the performance of an AI-integrated application?",
    category: "System Design",
    difficulty: "Mid",
    roles: ["Software Developer - AI", "AI Engineer"],
    stages: ["Technical Interview"],
    sampleAnswer: "Optimisation strategies: 1) Caching - semantic caching for similar queries; 2) Batching requests where possible; 3) Async processing for non-blocking UX; 4) Streaming for perceived performance; 5) Model selection - smaller models for simple tasks; 6) Prompt optimisation - shorter prompts reduce latency; 7) Parallel requests for independent AI calls; 8) Edge deployment for latency-sensitive features. Profile to find bottlenecks.",
    tips: "Discuss specific latency improvements you've achieved.",
  },
  {
    id: "sda-9",
    question: "Describe your experience integrating AI features into existing applications.",
    category: "Behavioral",
    difficulty: "Mid",
    roles: ["Software Developer - AI", "AI Engineer"],
    stages: ["Behavioral", "Technical Interview"],
    sampleAnswer: "Use STAR format: describe a specific integration project, challenges faced (legacy code, performance requirements, user expectations), solutions implemented (abstraction layers, gradual rollout, fallbacks), and outcomes (user adoption, performance metrics). Emphasise: working with existing codebases, stakeholder management, iterative improvement based on user feedback.",
    tips: "Prepare 2-3 specific examples with measurable outcomes.",
  },
  {
    id: "sda-10",
    question: "How do you approach building AI features that need to work offline or with poor connectivity?",
    category: "System Design",
    difficulty: "Senior",
    roles: ["Software Developer - AI", "AI Engineer"],
    stages: ["System Design"],
    sampleAnswer: "Strategies: 1) On-device models for critical features (ONNX, CoreML, TensorFlow Lite); 2) Intelligent caching of AI responses; 3) Request queuing for later processing; 4) Graceful degradation - simpler local logic as fallback; 5) Optimistic UI with background sync; 6) Compress models for mobile deployment; 7) Prioritise which features need offline support. Consider: model size, inference speed, battery impact, storage constraints.",
    tips: "Discuss specific on-device ML frameworks and their trade-offs.",
  },

  // Additional cross-role questions for new roles
  {
    id: "cross-1",
    question: "How do you stay current with the rapidly evolving AI landscape?",
    category: "Behavioral",
    difficulty: "Junior",
    roles: ["AI Engineer", "AI Governance Specialist", "Software Developer - AI", "Machine Learning Engineer", "AI Researcher"],
    stages: ["Phone Screen", "Behavioral"],
    sampleAnswer: "Methods: 1) Follow key researchers and practitioners on social media; 2) Read papers from major conferences (NeurIPS, ICML, ACL); 3) Hands-on experimentation with new tools and models; 4) Internal knowledge sharing sessions; 5) Online communities (Reddit, Discord, Twitter); 6) Newsletters (The Batch, TLDR AI); 7) Building side projects to learn. Focus on fundamentals that transfer across specific technologies.",
    tips: "Mention something specific you learned recently and how you applied it.",
  },
  {
    id: "cross-2",
    question: "Explain the AI development lifecycle from ideation to production.",
    category: "Case Study",
    difficulty: "Mid",
    roles: ["AI Engineer", "Software Developer - AI", "Machine Learning Engineer", "MLOps Engineer"],
    stages: ["Technical Interview"],
    sampleAnswer: "Stages: 1) Problem definition - is AI the right solution?; 2) Data assessment - availability, quality, labeling needs; 3) Proof of concept - validate feasibility; 4) Development - model training or integration; 5) Evaluation - offline metrics, user testing; 6) Deployment - infrastructure, monitoring; 7) Iteration - continuous improvement based on feedback. At each stage: documentation, stakeholder alignment, risk assessment. Not always linear - often iterative.",
    tips: "Discuss gates and decision points between stages.",
  },
  {
    id: "cross-3",
    question: "What considerations are important when choosing between building vs buying AI solutions?",
    category: "Case Study",
    difficulty: "Senior",
    roles: ["AI Engineer", "Software Developer - AI", "AI Governance Specialist"],
    stages: ["Technical Interview", "Final Round"],
    sampleAnswer: "Build considerations: competitive advantage, data privacy, customisation needs, long-term cost, team capability. Buy considerations: time to market, proven solution, maintenance burden, scaling. Hybrid: use APIs with custom fine-tuning. Evaluate: total cost of ownership, vendor lock-in, data ownership, reliability, support. Many organisations start with buy, then build differentiating capabilities.",
    tips: "Give examples of when you've made this decision and the reasoning.",
  },
  {
    id: "cross-4",
    question: "How do you communicate AI capabilities and limitations to non-technical stakeholders?",
    category: "Behavioral",
    difficulty: "Mid",
    roles: ["AI Engineer", "AI Governance Specialist", "Software Developer - AI", "Data Scientist"],
    stages: ["Behavioral", "Final Round"],
    sampleAnswer: "Approach: 1) Use analogies and concrete examples; 2) Demonstrate with prototypes rather than just explaining; 3) Be honest about limitations and failure cases; 4) Quantify uncertainty and confidence levels; 5) Relate to business outcomes, not technical metrics; 6) Use visualisations for complex concepts; 7) Address common misconceptions proactively. Build trust through transparency.",
    tips: "Share a specific example of successful stakeholder communication.",
  },
  {
    id: "cross-5",
    question: "What is your approach to debugging AI-related issues in production?",
    category: "MLOps & Deployment",
    difficulty: "Mid",
    roles: ["AI Engineer", "Software Developer - AI", "MLOps Engineer"],
    stages: ["Technical Interview"],
    sampleAnswer: "Debugging approach: 1) Reproduce the issue with specific inputs; 2) Check logs for errors, latency, token usage; 3) Isolate whether issue is input, model, or post-processing; 4) Test with variations of the input; 5) Compare against baseline/known good examples; 6) Check for data drift or prompt changes; 7) Use explainability tools if available. Maintain comprehensive logging of AI interactions for debugging.",
    tips: "Discuss specific debugging tools and techniques you use.",
  },
];

// Get all available questions
export function getAllQuestions(): InterviewQuestion[] {
  return INTERVIEW_QUESTIONS;
}

// Get questions filtered by criteria
export function getQuestionsByFilters(
  roles: Role[],
  difficulty: ExperienceLevel,
  stages: InterviewStage[],
  categories: QuestionCategory[]
): InterviewQuestion[] {
  return INTERVIEW_QUESTIONS.filter((q) => {
    const roleMatch = roles.length === 0 || roles.some((r) => q.roles.includes(r));
    const difficultyMatch = getDifficultyMatch(q.difficulty, difficulty);
    const stageMatch = stages.length === 0 || stages.some((s) => q.stages.includes(s));
    const categoryMatch = categories.length === 0 || categories.includes(q.category);

    return roleMatch && difficultyMatch && stageMatch && categoryMatch;
  });
}

// Difficulty matching logic - include questions at or below the specified level
function getDifficultyMatch(
  questionDifficulty: ExperienceLevel,
  targetDifficulty: ExperienceLevel
): boolean {
  const levels: ExperienceLevel[] = ["Junior", "Mid", "Senior", "Lead"];
  const questionLevel = levels.indexOf(questionDifficulty);
  const targetLevel = levels.indexOf(targetDifficulty);

  // Include questions at or one level below/above target
  return Math.abs(questionLevel - targetLevel) <= 1;
}

// Get categories with question counts
export function getCategoriesWithCounts(): { category: QuestionCategory; count: number }[] {
  const counts = new Map<QuestionCategory, number>();

  INTERVIEW_QUESTIONS.forEach((q) => {
    counts.set(q.category, (counts.get(q.category) || 0) + 1);
  });

  return QUESTION_CATEGORIES.map((category) => ({
    category,
    count: counts.get(category) || 0,
  }));
}
